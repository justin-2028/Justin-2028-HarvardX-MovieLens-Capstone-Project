---
title: "MovieLens Capstone Project"
author: "Justin Oh"
date: "08/13/2022"
output: pdf_document
---

# Introduction

Netflix, currently the most popular streaming service in the world, launched a competition in 2006 promising $1 million in prize money for the winning team. The objective of the challenge was to create an algorithm that outperformed Netflix's baseline algorithm, Cinematch, by 10%. The "Netflix Prize" contest highlighted the significance and financial benefits of research focused on improving existing recommendation systems, with companies such as social media firm Twitter hosting similar competitions of their own.

In the HarvardX: PH125.9x Data Science: Capstone course, an objective comparable to the "Netflix Prize" contest is presented to participants, albeit with the use of a different dataset. Rather than the datasets provided by Netflix, the 10M version of the MovieLens dataset provided by the research lab GroupLens will be used. As the name suggests, the dataset contains 10 million movie ratings, as well as 100,000 tag applications applied to 10,000 movies by 72,000 users.


## Goal of the Project

The MovieLens Project is designed to encompass the various machine learning and computational concepts learned from previous courses in the HarvardX Professional Data Science Certificate program. Using the MovieLens dataset provided, the goal of the project is to train an algorithm with a RMSE score less than 0.86490.

The Root Mean Square Error, or RMSE, is the value used to assess the algorithm's performance. Generally, a lower RMSE score indicates better performance, and vice versa. 

The following function computes the RMSE based on predicted and observed values:
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$


## Key Steps Performed

The MovieLens Project consisted of four key stages, and as such, the report has been divided into four distinct sections for improved readability.

1) Introduction: The context needed to understand the MovieLens Project is provided such as the definition of RMSE. Proceeded to download the MovieLens 10M dataset and created the "edx" and "valiation" subsets, which will serve as the training dataset and final hold-out test dataset, respectively.
2) Methods/Analysis: Performed exploratory analysis using the MovieLens dataset to clarify the influence that each feature has on movie ratings.
3) Results: Created multiple models that incorporate varying features of the MovieLens dataset while presenting the code associated. Upon doing so, the RMSE value of each model was calculated in search of the best performing algorithm. 
4) Conclusion: Provided a brief summary of my MovieLens Project as a whole, as well as conclusions that could be made hrough the results from the models created. Limitations specific to my project are elaborated on and future work is discussed for closure. 


## Overview of Provided Code

Below is the code provided by HarvardX:

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
```

As mentioned previously, the MovieLens dataset will be split into 2 subsets: a training subset (edX) and a testing subset (Validation).

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

## Examining the Data

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Load extra libraries for data analysis and visualization
if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
library(dslabs)
library(ggplot2)
library(lubridate)
```

After the data is loaded, we start the work by examining the data structure, data classifications and corresponding statistics of data set for better understanding of source data.

First check whether there is any missing value in data sets "edx" and "validation".

```{r message=FALSE, warning=FALSE}
#check any missing value in training data set "edx"
anyNA(edx)
#check any missing value in final hold-out test data set "validation"
anyNA(validation)
#list amount of observations and variables in training data set "edx"
dim(edx)
#list amount of observations and variables in final hold-out test data set "validation"
dim(validation)
```

The results of above checking indicate that no invalid or missing value in both data set "edx" and "validation". We observe that there are total 9,000,055 observations with 6 features in "edx" data set and 999,999 observations with 6 features in "validation" data set. Now we need to know what features are they and statistics of them.

Now we further explore the data set "edx" to know more about it.

Here are first 6 observations of data set "edx" for us to have preliminary view.
```{r echo=FALSE, message=FALSE, warning=FALSE}
head(edx)
```

Here is statistics of data set "edx".
```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(edx)
```

Now we know data set "edx" has initially 6 features of "userId", "movieId", "rating", "timestamp", "title" and "genres". Meanwhile, We notice that there is year information in the feature "title" which may need to be split for further analysis. Also, we may need to convert "timestamp" to rated year for further checking the time effect on rating. Moveover we notice that there are several types in feature "genres" for one single observation. We may need to split "genres" for better handling the genres effect on rating in upcoming data processing part. Now we know the average rating is 3.512 and minimum rating is 0.5.

##    Data Preprocessing
Before we look into more detailed data analysis, we will conduct minor data pre-processing on both "edx" and "validation" data sets. The data pre-processing tasks include converting feature "timestamp" to year rated, extracting year released from feature "title", calculating ages between year rated and year released and adding these 3 new features to original data sets.

```{r message=FALSE, warning=FALSE}
# convert timestamp to year rated and add it to edx
edx <- edx %>% mutate(year_rated = year(as_datetime(timestamp)))
# double check any invalid value after convertion of timestamp
unique(edx$year_rated)
# extract the year released from title and add it to edx
edx <- edx %>% mutate(year_released = as.numeric(str_sub(title,-5,-2)))
# double check any invalid value of year released
unique(edx$year_released)
# calculate the movie age when movie was rated and add it to edx
edx <- edx %>% mutate(ages = as.numeric(year_rated)-as.numeric(year_released))
# double check any invalid value of ages
unique(edx$ages)
```
We can see the result of data pre-processing is good except that 2 odd values of ages (-1 and -2) are observed. We are not sure whether the movies were rated before they were final released or they were just some data errors. Anyway, we plan to further check how much portion of observations with these odd values. 

```{r message=FALSE, warning=FALSE}
sum(edx$ages == -1)/nrow(edx)
sum(edx$ages == -2)/nrow(edx)
```

Now we know the portion of observations with odd values is quite small so it would not have big impact on our modeling and predication. Hence we just leave them as they are. 

Now we apply the same data pre-processing on data set "validation" too.

```{r message=FALSE, warning=FALSE}
# do the same data pre-processing for validation set
validation <- validation %>% mutate(year_rated = year(as_datetime(timestamp)))
validation <- validation %>% mutate(year_released = as.numeric(str_sub(title,-5,-2)))
validation <- validation %>% mutate(ages = as.numeric(year_rated)-as.numeric(year_released))
```

# Methods and Analysis


## Data Analysis

Six variables — "userID," "movieID," "rating," "timestamp," "title," and "genres" — are included in the edx subset. Each row corresponds to a single user's rating of a particular movie.

```{r head, echo = FALSE}
head(edx) %>%
  print.data.frame()
  
```

We can summarize the edx subset to confirm that there are no missing values.

```{r summary, echo = FALSE}
summary(edx)
```

The number of unique movies and users in the edx subset:

```{r, echo = FALSE}
edx %>%
summarize(n_users = n_distinct(userId), 
          n_movies = n_distinct(movieId))
```

We can see, in the figure below, that the rating distribution has a left-skewed distribution. Users have a preference to rate movies rather higher than lower. The rating 4 is the most common rating, followed by 3 and 5.

```{r rating_distribution, echo = FALSE}
edx %>%
  ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.25, color = "blue4") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  scale_y_continuous(breaks = c(seq(0, 3000000, 500000))) +
  ggtitle("Rating distribution")
  
```


We can plot the data and determine that some movies are rated more often than others, while some have very few  ratings and sometimes only one rating. Thus regularization and a penalty term will be applied to the models in this report.

```{r number_of_ratings_per_movie, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
count(movieId) %>%
ggplot(aes(n)) +
geom_histogram(bins = 30, color = "blue4") +
scale_x_log10() +
xlab("Number of ratings") +
  ylab("Number of movies") +
ggtitle("Number of ratings per movie")
```


As 20 movies that were rated only once appear to be obscure, predictions of future ratings for them will be difficult.


```{r obscure_movies, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
  group_by(movieId) %>%
  summarize(count = n()) %>%
  filter(count == 1) %>%
  left_join(edx, by = "movieId") %>%
  group_by(title) %>%
  summarize(rating = rating, n_rating = count) %>%
  slice(1:20) %>%
  knitr::kable()
  
```


The majority of users have rated below 100 movies, but also above 30 movies (a user penalty term will be included in the models).


```{r number_ratings_given_by_users, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
count(userId) %>%
ggplot(aes(n)) +
geom_histogram(bins = 30, color = "blue4") +
scale_x_log10() +
xlab("Number of ratings") + 
ylab("Number of users") +
ggtitle("Number of ratings given by users")
```


Furthermore, users differ vastly in how critical they are with their ratings. Some users tend to give much lower star ratings and some users tend to give higher star ratings than average. The visualization below includes only users that have rated at least 100 movies.


```{r Mean_movie_ratings_given_by_users, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, color = "blue4") +
  xlab("Mean rating") +
  ylab("Number of users") +
  ggtitle("Mean movie ratings given by users") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  theme_light()
  
```


## Modelling Approach

We write function, previously anticipated, that compute the RMSE, defined as follows:

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$


With N being the number of user/movie combinations and the sum occurring over all these combinations.
The RMSE is our measure of model accuracy.

The written function to compute the RMSE for vectors of ratings and their corresponding predictions is:

```{r RMSE_function2, echo = TRUE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The quality of the model will be assessed by the RMSE (the lower the better).


### I. Naive Model

Creating a prediction system that only utilizes the sample mean. This implies that every prediction is the sample average. 

This is a model based approach which assumes the same rating for all movie with all differences explained by random variation :
$$ Y_{u, i} = \mu + \epsilon_{u, i} $$
with $\epsilon_{u,i}$ independent error sample from the same distribution centered at 0 and $\mu$ the “true” rating for all movies. This very simple model makes the assumption that all differences in movie ratings are explained by random variation alone. We know that the estimate that minimize the RMSE is the least square estimate of $Y_{u,i}$ , in this case, is the average of all ratings:

```{r, echo = TRUE}
mu <- mean(edx$rating)
mu
```

Getting the first naive RMSE:
The  resulting RMSE using this approach is quite high.

```{r naive_rmse, echo = TRUE}
naive_rmse <- RMSE(validation$rating, mu)
naive_rmse
```

Here, we represent results table with the first RMSE:

```{r rmse_results1, echo = TRUE}
rmse_results <- data_frame(method = "Naive model", RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```

This give us our baseline RMSE to compare with next modelling approaches.

### II.  Movie Effect Model

Higher ratings are mostly linked to popular movies among users and the opposite is true for unpopular movies. 

We compute the estimated deviation of each movies’ mean rating from the total mean of all movies $\mu$. The resulting variable is called "b" ( as bias ) for each movie "i" $b_{i}$, that represents average ranking for movie $i$:
$$Y_{u, i} = \mu +b_{i}+ \epsilon_{u, i}$$

The histogram is left skewed, implying that more movies have negative effects


```{r Number_of_movies_with_the computed_b_m, echo = TRUE, fig.height=3, fig.width=4}
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_m = mean(rating - mu))
movie_avgs %>% qplot(b_m, geom ="histogram", bins = 10, data = ., color = I("blue4"),
ylab = "Number of movies", main = "Number of movies with the computed b_m")
```


This is called the penalty term movie effect.

Our prediction improve once we predict using this model.

```{r predicted_ratings, echo = TRUE}
predicted_ratings <- mu +  validation %>%
  left_join(movie_avgs, by='movieId') %>%
  pull(b_m)
model_1_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie effect model",  
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()
```


So we have predicted movie rating based on the fact that movies are rated differently by adding the computed $b_{i}$ to $\mu$. If an individual movie is on average rated worse that the average rating of all movies $\mu$ , we predict that it will rated lower that $\mu$ by $b_{i}$, the difference of the individual movie average from the total average.


### III. Movie + User Effects Model

It is understood that users may have a tendency to rate movies higher or lower than the overall mean. Let's add this into the model. First we'll calculate the bias for each user:

$$b_{u} = Mean_{user} - \mu$$
Then we'll combine the bias of a user, with the bias of a film and add both to the overall mean for a combined bias rating for each unique combination of a user rating for a given film.

$$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$$
  
```{r, echo = TRUE}
user_effects<- edx %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating - mu - b_m))
user_effects%>% qplot(b_u, geom ="histogram", bins = 30, data = ., color = I("blue4"))
```


We compute an approximation by computing $\mu$ and $b_{i}$, and estimating  $b_{u}$, as the average of $$Y_{u, i} - \mu - b_{i}$$
  
```{r user_effects, echo = TRUE}
user_effects <- edx %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_m))
```

We can now construct predictors and see RMSE improves:
  
  
```{r model_2_rmse, echo = TRUE}
predicted_ratings <- validation%>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_effects, by='userId') %>%
  mutate(prediction = mu + b_m + b_u) %>%
  pull(prediction)
model_2_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie and user effect model",  
                                     RMSE = model_2_rmse))
rmse_results %>% knitr::kable()
```


Our rating predictions further reduced the RMSE. But we made stil mistakes on our first model (using only movies). The supposes “best “ and “worst “movie were rated by few users, in most cases just one user. These movies were mostly obscure ones. This is because with a few users, we have more uncertainty. Therefore larger estimates of $b_{i}$, negative or positive, are more likely.
Large errors can increase our RMSE. 

### IV. Regularized Movie + User Effects Model

The general idea behind regularization is to constrain the total variability of the effect size. As lambda is a tuning parameter, we can use cross-validation to choose optimized one for model building and performance evaluation.

```{r message=FALSE, warning=FALSE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
mu_reg <- mean(edx$rating)
#regulation movie effect
b_m_reg <- edx %>%
group_by(movieId) %>%
summarize(b_m_reg = sum(rating - mu_reg)/(n()+l))
#regulation user effect
b_u_reg <- edx %>%
left_join(b_m_reg, by="movieId") %>%
group_by(userId) %>%
summarize(b_u_reg = sum(rating - b_m_reg - mu_reg)/(n()+l))
#calculating predicted ratings
predicted_ratings_b_m_u <-
validation %>%
left_join(b_m_reg, by = "movieId") %>%
left_join(b_u_reg, by = "userId") %>%
mutate(prediction = mu_reg + b_m_reg + b_u_reg) %>%
.$prediction
return(RMSE(validation$rating,predicted_ratings_b_m_u))
})
qplot(lambdas, rmses)
#For the full model, the optimal lambda is given as
lambda <- lambdas[which.min(rmses)]
lambda
#calculating regularization model RMSE: model_m_u_reg_rmse
model_m_u_reg_rmse <- min(rmses)
model_m_u_reg_rmse
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Regularization Model",
                                     RMSE = model_m_u_reg_rmse))
rmse_results %>% knitr::kable()
```

Now we finally achieve the goal of RMSE which is less than 0.86490 by model of regularization of movie and user effects. The new RMSE is 0.8648. 

\pagebreak

# Results

The RMSE values of all the represented models are the following:

```
rmse_results %>% knitr::kable()
```

We therefore found the lowest value of RMSE that is 0.8648170.

So we can confirm that the final model for our project is the Regularized movie and user effect model:

$$Y_{u, i} = \mu + b_{m} + b_{u} + \epsilon_{u, m}$$

As expected, the RMSE decreased while the model increases complexity. We can even discuss if a good alternative is to increase the parameters of the models like inlude genres, however to split the genres in the data we need more advanced hardware. 

# Conclusion

In order to construct a movie recommendation algorithm with a sufficiently low RMSE, I have employed an iterative applied machine learning approach. Beginning with a naive approach, I developed models that branched out to MovieLens features such as movie and user effects, and applied the regularization technique to further cut down the RMSE value.

Considering the fact that only two predictors, "movieId" and "userId", were used to obtain the desired RMSE value, there is far more potential that can be realized through the incorporation of other predictors such as "genres" and "releaseYear". A drastic RMSE reduction would also be possible with the use of a larger dataset. The MovieLens 10M dataset could be swapped for the 25M dataset offered on the GroupLens website to nearly triple the number of ratings at my disposal.

While there is clear room for improvement, the Regularized Movie + User Effects Model satisfies the primary goal of the MovieLens Project, making this venture an overall success.

## Limitations

My MovieLens Project encounter several limitations that I hope to address in the future, perhaps through the assistance of my peer reviewers. 

1) Software/Hardware Limitations: I have attempted several advanced machine learning models that could have reduced the RMSE further, such as k-NN models, but the lackluster processing power and RAM of my laptop has currently made it difficult to do so. 
2) Exploratory Limitations: A higher understanding of the MovieLens dataset would have allowed me to fine-tune the models further.

## Future Works

As summarized above, my attempts at creating a better model has been hampered by the limitations of my device and my understanding of the MovieLens dataset. However, I intend on trying additional models that show much promise.

1) Installing the recosystem package to utilize Matrix Factorization.
2) Utilize PCA (Principal Component Analysis) to improve visualization of the data.

## Acknowledgements

I would like to thank Professor Rafael Irizarry, as well as the countless educators, moderators, and supervisors that have contributed to the HarvardX Professional Data Science Certificate Program. Their collective efforts have provided one of the best remote learning experiences I have had to date, and I am extremely grateful to have studied the R programming language under such accredited curriculum. I also share my gratitude and heartfelt thanks to my fellow peers, whose discussions on the interactive forums have given me invaluable insight throughout the journey. Lastly, I would like to acknowledge my parents, who have never stopped supporting my dreams.
