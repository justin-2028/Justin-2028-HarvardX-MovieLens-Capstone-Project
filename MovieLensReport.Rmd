---
title: "MovieLens Capstone Project"
author: "Justin Oh"
date: "08/13/2022"
output: pdf_document
---

# Introduction

Netflix, currently the most popular streaming service in the world, launched a competition in 2006 promising $1 million in prize money for the winning team. The objective of the challenge was to create an algorithm that outperformed Netflix's baseline algorithm, Cinematch, by 10%. The "Netflix Prize" contest highlighted the significance and financial benefits of research focused on improving existing recommendation systems, with companies such as social media firm Twitter hosting similar competitions of their own.

In the HarvardX: PH125.9x Data Science: Capstone course, an objective comparable to the "Netflix Prize" contest is presented to participants, albeit with the use of a different dataset. Rather than the datasets provided by Netflix, the 10M version of the MovieLens dataset provided by the research lab GroupLens will be used. As the name suggests, the dataset contains 10 million movie ratings, as well as 100,000 tag applications applied to 10,000 movies by 72,000 users.


## Goal of the Project

The MovieLens Project is designed to encompass the various machine learning and computational concepts learned from previous courses in the HarvardX Professional Data Science Certificate program. Using the MovieLens dataset provided, the goal of the project is to train an algorithm with a RMSE score less than 0.86490.

The Root Mean Square Error, or RMSE, is the value used to assess the algorithm's performance. Generally, a lower RMSE score indicates better performance, and vice versa. 

The following function computes the RMSE based on predicted and observed values:
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$


## Key Steps Performed

The MovieLens Project consisted of four key stages, and as such, the report has been divided into four distinct sections for improved readability.

1) Introduction: The context needed to understand the MovieLens Project is provided such as the definition of RMSE. Proceeded to download the MovieLens 10M dataset and created the "edx" and "valiation" subsets, which will serve as the training dataset and final hold-out test dataset, respectively.
2) Methods/Analysis: Performed exploratory analysis using the MovieLens dataset to clarify the influence that each feature has on movie ratings.
3) Results: Created multiple models that incorporate varying features of the MovieLens dataset while presenting the code associated. Upon doing so, the RMSE value of each model was calculated in search of the best performing algorithm. 
4) Conclusion: Provided a brief summary of my MovieLens Project as a whole, as well as conclusions that could be made hrough the results from the models created. Limitations specific to my project are elaborated on and future work is discussed for closure. 


## Overview of Provided Code

Below is the code provided by HarvardX:

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
```

As mentioned previously, the MovieLens dataset will be split into 2 subsets: a training subset (edX) and a testing subset (Validation).

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

## Examining the Data

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Load extra libraries for data analysis and visualization
if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
library(dslabs)
library(ggplot2)
library(lubridate)
```

After the data is loaded, we will examine the structure, classifications and corresponding statistics of our data set for better understanding of the source data.

We will first check to see whether there are any missing values both the "edx" and "validation" subsets.

```{r message=FALSE, warning=FALSE}
#check any missing value in training data set "edx"
anyNA(edx)
#check any missing value in final hold-out test data set "validation"
anyNA(validation)
#list amount of observations and variables in training data set "edx"
dim(edx)
#list amount of observations and variables in final hold-out test data set "validation"
dim(validation)
```

The results above show that neither the data set "edx" nor the "validation" include any incorrect or missing values. We can see that the "edx" data set has a total of 9,000,055 observations with 6 features, whereas the "validation" data set contains 999,999 observations with 6 features. We now need to be aware of their various statistics and distinguishable qualities.

To learn more about the data set "edx," we are now exploring it further.

Here are the first six observations from the data set "edx" so that we can get a quick overview.

```{r echo=FALSE, message=FALSE, warning=FALSE}
head(edx)
```

Here is statistics of data set "edx".
```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(edx)
```

As of right now, we are aware that the data set "edx" initially included six features: "userId," "movieId," "rating," "timestamp," "title," and "genres." In the meanwhile, we note that the feature's "title" contains year information; this information may need to be divided for additional analysis. Additionally, in order to further examine the impact of time on rating, "timestamp" may need to be converted to rated year. Moving on, we see that there are several "genres" of feature types for a single observation. To handle the impact of genres on ratings in the next data processing portion, we might need to divide "genres." We now know that the minimum rating is 0.5 and the average rating is 3.512.

##    Data Preprocessing
We will perform some minimal data pre-processing on both the "edx" and "validation" data sets before we go into further in-depth data analysis. As part of the data pre-processing steps, the feature "timestamp" is converted to the year rated, the year released is extracted from the feature "title", the age between the year rated and year released is calculated, and all 3 new features are added to the original data sets.

```{r message=FALSE, warning=FALSE}
# convert timestamp to year rated and add it to edx
edx <- edx %>% mutate(year_rated = year(as_datetime(timestamp)))
# double check any invalid value after convertion of timestamp
unique(edx$year_rated)
# extract the year released from title and add it to edx
edx <- edx %>% mutate(year_released = as.numeric(str_sub(title,-5,-2)))
# double check any invalid value of year released
unique(edx$year_released)
# calculate the movie age when movie was rated and add it to edx
edx <- edx %>% mutate(ages = as.numeric(year_rated)-as.numeric(year_released))
# double check any invalid value of ages
unique(edx$ages)
```
We can see that the pre-processing of the data produced decent results with the exception of the observation of 2 odd ages (-1 and -2). We're not sure if the movies were rated before they were officially released or if there were data issues. We need to further investigate the observations that have these peculiar values.

```{r message=FALSE, warning=FALSE}
sum(edx$ages == -1)/nrow(edx)
sum(edx$ages == -2)/nrow(edx)
```

The number of observations with odd values are not significant enough to influence modeling to a concerning degree, so we will keep these values for now. 

We will apply the same process on the "validation" dataset as well.

```{r message=FALSE, warning=FALSE}
# do the same data pre-processing for validation set
validation <- validation %>% mutate(year_rated = year(as_datetime(timestamp)))
validation <- validation %>% mutate(year_released = as.numeric(str_sub(title,-5,-2)))
validation <- validation %>% mutate(ages = as.numeric(year_rated)-as.numeric(year_released))
```

# Methods and Analysis


## Data Analysis

Six variables — "userID," "movieID," "rating," "timestamp," "title," and "genres" — are included in the edx subset. Each row corresponds to a single user's rating of a particular movie.

```{r head, echo = FALSE}
head(edx) %>%
  print.data.frame()
  
```

Using the summarize function, we can check the edx subset for any missing values.

```{r summary, echo = FALSE}
summary(edx)
```

We can also calculate the number of unique movies and users in the edx subset:

```{r, echo = FALSE}
edx %>%
summarize(n_users = n_distinct(userId), 
          n_movies = n_distinct(movieId))
```

The figure below visually illustrates the left-skewed distribution of the rating scale. Users prefer to give movies better ratings than lower ones. The most prevalent rating is 4, followed by 3 and 5.

```{r rating_distribution, echo = FALSE}
edx %>%
  ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.25, color = "blue4") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  scale_y_continuous(breaks = c(seq(0, 3000000, 500000))) +
  ggtitle("Rating distribution")
  
```


We can visualize the data and see that some films receive ratings more frequently than others, while other films receive extremely few or even no ratings at all. The models in this report will be subjected to regularization and a penalty term as a result.

```{r number_of_ratings_per_movie, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
count(movieId) %>%
ggplot(aes(n)) +
geom_histogram(bins = 30, color = "blue4") +
scale_x_log10() +
xlab("Number of ratings") +
  ylab("Number of movies") +
ggtitle("Number of ratings per movie")
```


It will be challenging to anticipate future ratings for the 20 films that have only received one rating given their obscurity.

```{r obscure_movies, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
  group_by(movieId) %>%
  summarize(count = n()) %>%
  filter(count == 1) %>%
  left_join(edx, by = "movieId") %>%
  group_by(title) %>%
  summarize(rating = rating, n_rating = count) %>%
  slice(1:20) %>%
  knitr::kable()
  
```


The vast majority of people have given between 30 to 100 movie ratings, and as a result, a user penalty term will be included in the models.

```{r number_ratings_given_by_users, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
count(userId) %>%
ggplot(aes(n)) +
geom_histogram(bins = 30, color = "blue4") +
scale_x_log10() +
xlab("Number of ratings") + 
ylab("Number of users") +
ggtitle("Number of ratings given by users")
```

As we can see, the degree to which people are critical of a movie (which is reflected in their rating) also varies greatly. Some people have a tendency to rate items with far less stars than the typical user, and vice versa. Only users with at least 100 movie ratings are shown in the following graphic below.

```{r Mean_movie_ratings_given_by_users, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, color = "blue4") +
  xlab("Mean rating") +
  ylab("Number of users") +
  ggtitle("Mean movie ratings given by users") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  theme_light()
  
```


## Modelling Approach

We write function, previously anticipated, that compute the RMSE, defined as follows:

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$


N represents the number of user/movie combinations and the sum occurring over all these combinations.
Our metric for model precision is the RMSE.

The following written function computes the RMSE through the vectors of ratings and their corresponding predictions:

```{r RMSE_function2, echo = TRUE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The RMSE will judge the model's level of quality. As mentioned before, a lower RMSE score indicates better results.

### 1) Naive Model

We will develop a prediction system that just makes use of the sample mean. This suggests that each prediction is the sample average.

This model-based approacb assumes that all movies will receive the same rating and that any discrepancies will be due to random variation:
$$ Y_{u, i} = \mu + \epsilon_{u, i} $$
with $\epsilon_{u,i}$ independent error sample from the same distribution centered at 0 and $\mu$ the “true” rating for all movies. This very simple model makes the assumption that all differences in movie ratings are explained by random variation alone. We know that the estimate that minimizes the RMSE is the least square estimate of $Y_{u,i}$ , which is the average of all ratings:

```{r, echo = TRUE}
mu <- mean(edx$rating)
mu
```

Here, we solve for the first naive RMSE.
The  resulting RMSE using this approach is higher than we would like.

```{r naive_rmse, echo = TRUE}
naive_rmse <- RMSE(validation$rating, mu)
naive_rmse
```

Here, we represent results table with the first RMSE:

```{r rmse_results1, echo = TRUE}
rmse_results <- data_frame(method = "Naive model", RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```

While this is far higher than the 0.86490 RMSE objective, this provides us with our baseline RMSE for comparison with future modeling strategies.

### 2) Movie Effect Model

Popular movies typically receive higher ratings from people, whereas unpopular movies typically receive lower ratings.

We will compute the estimated deviation of each movies’ mean rating from the total mean of all movies $\mu$. The resulting variable is called "b" ( as bias ) for each movie "i" $b_{i}$, that represents average ranking for movie $i$:
$$Y_{u, i} = \mu +b_{i}+ \epsilon_{u, i}$$

The created histogram is left skewed, suggesting that more movies have negative effects.


```{r Number_of_movies_with_the computed_b_m, echo = TRUE, fig.height=3, fig.width=4}
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_m = mean(rating - mu))
movie_avgs %>% qplot(b_m, geom ="histogram", bins = 10, data = ., color = I("blue4"),
ylab = "Number of movies", main = "Number of movies with the computed b_m")
```


Our prediction improve once we predict using this model:

```{r predicted_ratings, echo = TRUE}
predicted_ratings <- mu +  validation %>%
  left_join(movie_avgs, by='movieId') %>%
  pull(b_m)
model_1_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie effect model",  
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()
```


By adding the computed $b_{i}$ to $\mu$, we are able to predict movie ratings. If an individual movie is on average rated worse that the average rating of all movies $\mu$ , we predict that it will rated lower that $\mu$ by $b_{i}$, the difference of the individual movie average from the total average.


### 3) Movie + User Effects Model

Users may have a tendency to rate movies higher or lower than the overall mean, so we can take this into account by incorporating it into the next model. First we will calculate the bias for each user:

$$b_{u} = Mean_{user} - \mu$$
Then we'll combine the bias of a user, with the bias of a film and add both to the overall mean for a combined bias rating for each unique combination of a user rating for a given film.

$$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$$
  
```{r, echo = TRUE}
user_effects<- edx %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating - mu - b_m))
user_effects%>% qplot(b_u, geom ="histogram", bins = 30, data = ., color = I("blue4"))
```


By computing $\mu$ and $b_{i}$, we can compute an approximation, and estimate $b_{u}$ as the average of $$Y_{u, i} - \mu - b_{i}$$
  
```{r user_effects, echo = TRUE}
user_effects <- edx %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_m))
```

Now, we can create predictors and see how the RMSE improves:
  
  
```{r model_2_rmse, echo = TRUE}
predicted_ratings <- validation%>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_effects, by='userId') %>%
  mutate(prediction = mu + b_m + b_u) %>%
  pull(prediction)
model_2_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie and user effect model",  
                                     RMSE = model_2_rmse))
rmse_results %>% knitr::kable()
```


The Movie Effect Model was flawed as we only accounted for the "movieID" predictor, so with the addition of the "userID" predictor, our third model's rating predictions achieved a far lower RMSE value. Howeever, fewer users results in increased uncertainty of our predictions. Due to this, larger estimates of $b_{i}$, negative or positive, are more likely. To prevent these larger estimates from negatively impacting our RMSE, we will create a regularized model to avoid such circumstances.

### 4) Regularized Movie + User Effects Model

Regularization is typically used to limit the overall fluctuation, or variability, of the effect size. Lambda is a tuning parameter, so we can use cross-validation to select the best one for model construction and performance assessment.

```{r message=FALSE, warning=FALSE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
mu_reg <- mean(edx$rating)
#regulation movie effect
b_m_reg <- edx %>%
group_by(movieId) %>%
summarize(b_m_reg = sum(rating - mu_reg)/(n()+l))
#regulation user effect
b_u_reg <- edx %>%
left_join(b_m_reg, by="movieId") %>%
group_by(userId) %>%
summarize(b_u_reg = sum(rating - b_m_reg - mu_reg)/(n()+l))
#calculating predicted ratings
predicted_ratings_b_m_u <-
validation %>%
left_join(b_m_reg, by = "movieId") %>%
left_join(b_u_reg, by = "userId") %>%
mutate(prediction = mu_reg + b_m_reg + b_u_reg) %>%
.$prediction
return(RMSE(validation$rating,predicted_ratings_b_m_u))
})
qplot(lambdas, rmses)
#For the full model, the optimal lambda is given as
lambda <- lambdas[which.min(rmses)]
lambda
#calculating regularization model RMSE: model_m_u_reg_rmse
model_m_u_reg_rmse <- min(rmses)
model_m_u_reg_rmse
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Regularization Model",
                                     RMSE = model_m_u_reg_rmse))
rmse_results %>% knitr::kable()
```

Through the Regularized Movie + User Effects Model, we have finally achieved the target RMSE (less than 0.86490) with a RMSE value of 0.8648170. 

\pagebreak

# Results

The table below shows the RMSE values achieved by all four models examined over the course of this report:

```
rmse_results %>% knitr::kable()
```

We drastically reduced the initial RMSE of 1.0612018 to 0.8648170 with the Regularized Movie + User Effect Model:

$$Y_{u, i} = \mu + b_{m} + b_{u} + \epsilon_{u, m}$$

# Conclusion

In order to construct a movie recommendation algorithm with a sufficiently low RMSE, I have employed an iterative applied machine learning approach. Beginning with a naive approach, I developed models that branched out to MovieLens features such as movie and user effects, and applied the regularization technique to further cut down the RMSE value.

Considering the fact that only two predictors, "movieId" and "userId", were used to obtain the desired RMSE value, there is far more potential that can be realized through the incorporation of other predictors such as "genres" and "releaseYear". A drastic RMSE reduction would also be possible with the use of a larger dataset. The MovieLens 10M dataset could be swapped for the 25M dataset offered on the GroupLens website to nearly triple the number of ratings at my disposal.

While there is clear room for improvement, the Regularized Movie + User Effects Model satisfies the primary goal of the MovieLens Project, making this venture an overall success.

## Limitations

My MovieLens Project encounter several limitations that I hope to address in the future, perhaps through the assistance of my peer reviewers. 

1) Software/Hardware Limitations: I have attempted several advanced machine learning models that could have reduced the RMSE further, such as k-NN models, but the lackluster processing power and RAM of my laptop has currently made it difficult to do so. 
2) Exploratory Limitations: A higher understanding of the MovieLens dataset would have allowed me to fine-tune the models further.

## Future Works

As summarized above, my attempts at creating a better model has been hampered by the limitations of my device and my understanding of the MovieLens dataset. However, I intend on trying additional models that show much promise.

1) Installing the recosystem package to utilize Matrix Factorization.
2) Utilize PCA (Principal Component Analysis) to improve visualization of the data.

## Acknowledgements

I would like to thank Professor Rafael Irizarry, as well as the countless educators, moderators, and supervisors that have contributed to the HarvardX Professional Data Science Certificate Program. Their collective efforts have provided one of the best remote learning experiences I have had to date, and I am extremely grateful to have studied the R programming language under such accredited curriculum. I also share my gratitude and heartfelt thanks to my fellow peers, whose discussions on the interactive forums have given me invaluable insight throughout the journey. Lastly, I would like to acknowledge my parents, who have never stopped supporting my dreams.
