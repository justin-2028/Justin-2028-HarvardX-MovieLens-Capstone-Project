---
title: "MovieLens Capstone Project"
author: "Justin Oh"
date: "08/08/2022"
output: pdf_document
---
  
# Overview
  
  This project is related to the MovieLens Project of the HarvardX: PH125.9x Data Science: Capstone course. The present report start with a general idea of the project and by representing its objective.

Then the given dataset will be prepared and setup. An exploratory data analysis is carried out in order to develop a machine learning algorithm that could predict movie ratings until a final model. Results will be explained. Finally the report ends with some concluding remarks.


## Introduction

Statistical and knowledge discovery techniques are applied to the problem of producing product recommendations or ratings through recommmender systems and on the basis of previously recorded data. In the present report, the products are the movies.

The present report covers the 10M version of the movieLens dataset available here <https://grouplens.org/datasets/movielens/10m/>.

The Netflix prize (i.e. challenge to improve the predictions of Netfix's movie recommender system by above 10% in terms of the root mean square error) reflects the importance and economic impact of research in the recommendation systems field. 


## Aim of the project

The goal is to train a machine learning algorithm using the inputs of a provided training subset to predict movie ratings in a validation set.
The predicted user ratings will be in the range from 0.5 to 5.

The value used to evaluate the algorithm performance is the Root Mean Square Error, or RMSE. (Low RMSE -> Better Performance)

The function that computes the RMSE for vectors of ratings and their corresponding predictors will be the following:
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$

Finally, the best resulting model will be used to predict the movie ratings.


## Dataset

Code provided by edx staff to download an create edx dataset:

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
```

The Movielens dataset will be splitted into 2 subset, a training subset (edx), and a testing subset (Validation):

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


# Methods and Analysis


## Data Analysis

The edx subset contains  six variables “userID”, “movieID”, “rating”, “timestamp”, “title”, and “genres”. Each row represent a single rating of a user for a single movie.

```{r head, echo = FALSE}
head(edx) %>%
  print.data.frame()
  
```

We can summarize the edx subset to confirm that there are no missing values.

```{r summary, echo = FALSE}
summary(edx)
```

The number of unique movies and users in the edx subset:

```{r, echo = FALSE}
edx %>%
summarize(n_users = n_distinct(userId), 
          n_movies = n_distinct(movieId))
```

We can see, in the figure below, that the rating distribution has a left-skewed distribution. Users have a preference to rate movies rather higher than lower. The rating 4 is the most common rating, followed by 3 and 5.

```{r rating_distribution, echo = FALSE}
edx %>%
  ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.25, color = "black") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  scale_y_continuous(breaks = c(seq(0, 3000000, 500000))) +
  ggtitle("Rating distribution")
  
```


We can plot the data and determine that some movies are rated more often than others, while some have very few  ratings and sometimes only one rating. Thus regularization and a penalty term will be applied to the models in this report.

```{r number_of_ratings_per_movie, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
count(movieId) %>%
ggplot(aes(n)) +
geom_histogram(bins = 30, color = "black") +
scale_x_log10() +
xlab("Number of ratings") +
  ylab("Number of movies") +
ggtitle("Number of ratings per movie")
```


As 20 movies that were rated only once appear to be obscure, predictions of future ratings for them will be difficult.


```{r obscure_movies, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
  group_by(movieId) %>%
  summarize(count = n()) %>%
  filter(count == 1) %>%
  left_join(edx, by = "movieId") %>%
  group_by(title) %>%
  summarize(rating = rating, n_rating = count) %>%
  slice(1:20) %>%
  knitr::kable()
  
```


The majority of users have rated below 100 movies, but also above 30 movies (a user penalty term will be included in the models).


```{r number_ratings_given_by_users, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
count(userId) %>%
ggplot(aes(n)) +
geom_histogram(bins = 30, color = "black") +
scale_x_log10() +
xlab("Number of ratings") + 
ylab("Number of users") +
ggtitle("Number of ratings given by users")
```


Furthermore, users differ vastly in how critical they are with their ratings. Some users tend to give much lower star ratings and some users tend to give higher star ratings than average. The visualization below includes only users that have rated at least 100 movies.


```{r Mean_movie_ratings_given_by_users, echo = TRUE, fig.height=4, fig.width=5}
edx %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, color = "black") +
  xlab("Mean rating") +
  ylab("Number of users") +
  ggtitle("Mean movie ratings given by users") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  theme_light()
  
```


## Modelling Approach

We write function, previously anticipated, that compute the RMSE, defined as follows:

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$


With N being the number of user/movie combinations and the sum occurring over all these combinations.
The RMSE is our measure of model accuracy.

The written function to compute the RMSE for vectors of ratings and their corresponding predictions is:

```{r RMSE_function2, echo = TRUE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The quality of the model will be assessed by the RMSE (the lower the better).


### I. Naive Model

Creating a prediction system that only utilizes the sample mean. This implies that every prediction is the sample average. 

This is a model based approach which assumes the same rating for all movie with all differences explained by random variation :
$$ Y_{u, i} = \mu + \epsilon_{u, i} $$
with $\epsilon_{u,i}$ independent error sample from the same distribution centered at 0 and $\mu$ the “true” rating for all movies. This very simple model makes the assumption that all differences in movie ratings are explained by random variation alone. We know that the estimate that minimize the RMSE is the least square estimate of $Y_{u,i}$ , in this case, is the average of all ratings:

```{r, echo = TRUE}
mu <- mean(edx$rating)
mu
```

Getting the first naive RMSE:
The  resulting RMSE using this approach is quite high.

```{r naive_rmse, echo = TRUE}
naive_rmse <- RMSE(validation$rating, mu)
naive_rmse
```

Here, we represent results table with the first RMSE:

```{r rmse_results1, echo = TRUE}
rmse_results <- data_frame(method = "Naive model", RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```

This give us our baseline RMSE to compare with next modelling approaches.

### II.  Movie Effect Model

Higher ratings are mostly linked to popular movies among users and the opposite is true for unpopular movies. 

We compute the estimated deviation of each movies’ mean rating from the total mean of all movies $\mu$. The resulting variable is called "b" ( as bias ) for each movie "i" $b_{i}$, that represents average ranking for movie $i$:
$$Y_{u, i} = \mu +b_{i}+ \epsilon_{u, i}$$

The histogram is left skewed, implying that more movies have negative effects


```{r Number_of_movies_with_the computed_b_i, echo = TRUE, fig.height=3, fig.width=4}
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"),
ylab = "Number of movies", main = "Number of movies with the computed b_i")
```


This is called the penalty term movie effect.

Our prediction improve once we predict using this model.

```{r predicted_ratings, echo = TRUE}
predicted_ratings <- mu +  validation %>%
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
model_1_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie effect model",  
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()
```


So we have predicted movie rating based on the fact that movies are rated differently by adding the computed $b_{i}$ to $\mu$. If an individual movie is on average rated worse that the average rating of all movies $\mu$ , we predict that it will rated lower that $\mu$ by $b_{i}$, the difference of the individual movie average from the total average.


### III. Movie + User Effects Model

It is understood that users may have a tendency to rate movies higher or lower than the overall mean. Let's add this into the model. First we'll calculate the bias for each user:

$$b_{u} = Mean_{user} - \mu$$
Then we'll combine the bias of a user, with the bias of a film and add both to the overall mean for a combined bias rating for each unique combination of a user rating for a given film.

$$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$$
  
```{r, echo = TRUE}
user_avgs<- edx %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating - mu - b_i))
user_avgs%>% qplot(b_u, geom ="histogram", bins = 30, data = ., color = I("black"))
```


We compute an approximation by computing $\mu$ and $b_{i}$, and estimating  $b_{u}$, as the average of $$Y_{u, i} - \mu - b_{i}$$
  
```{r user_avgs, echo = TRUE}
user_avgs <- edx %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

```

We can now construct predictors and see RMSE improves:
  
  
```{r model_2_rmse, echo = TRUE}
predicted_ratings <- validation%>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
model_2_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie and user effect model",  
                                     RMSE = model_2_rmse))
rmse_results %>% knitr::kable()
```


Our rating predictions further reduced the RMSE. But we made stil mistakes on our first model (using only movies). The supposes “best “ and “worst “movie were rated by few users, in most cases just one user. These movies were mostly obscure ones. This is because with a few users, we have more uncertainty. Therefore larger estimates of $b_{i}$, negative or positive, are more likely.
Large errors can increase our RMSE. 


### IV. Movie + User Effects + Age Effects Model

Similar to the movie effect, we now further add the bias of user effect (b_u) to the movie effect model.

```{r message=FALSE, warning=FALSE}
#calculating and plot b_u distribution
user_effects <- edx %>% 
  left_join(movie_effects, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu_hat - b_m))
user_effects %>% qplot(b_u, geom ="histogram", bins = 10, data = ., color = I("blue")) 
#calculating predicted ratings
predicted_ratings_m_u <- validation %>% 
  left_join(movie_effects, by='movieId') %>%
  left_join(user_effects, by='userId') %>%
  mutate(prediction = mu_hat + b_m + b_u) %>%
  pull(prediction)
#calculating model_m_rmse with consideration of movie + user effects  
model_m_u_rmse <- RMSE(validation$rating,predicted_ratings_m_u)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="Movie + User Effects Model",  
                                     RMSE = model_m_u_rmse))
rmse_results %>% knitr::kable()
```

Again, the new RMSE by adding movieId and userId together drops from 0.9439 to 0.8653. The movie and user effect model further improves predication accuracy around 18.5% compared with "Only by average rating" model.

In data analysis part, we estimate the time effect would be weak. What will new RMSE be by adding movie age on top of movie and user effect?

### V. Movie + User Effects Regularization Model

We now further add the bias of movie age effect (b_a) to the movie + user effect model.

```{r message=FALSE, warning=FALSE}
#calculating and plot b_a distribution
ages_effects <- edx %>% 
  left_join(movie_effects, by='movieId') %>%
  left_join(user_effects, by='userId') %>%
  group_by(ages) %>%
  summarize(b_a = mean(rating - mu_hat - b_m - b_u))
ages_effects %>% qplot(b_a, geom ="histogram", bins = 10, data = ., color = I("blue"))
#calculating predicted ratings
predicted_ratings_m_u_a <- validation %>% 
  left_join(movie_effects, by='movieId') %>%
  left_join(user_effects, by='userId') %>%
  left_join(ages_effects, by='ages') %>%
  mutate(prediction = mu_hat + b_m + b_u + b_a) %>%
  pull(prediction)
#calculating model_m_rmse with consideration of movie + user + age effects  
model_m_u_a_rmse <- RMSE(validation$rating,predicted_ratings_m_u_a)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="Movie + User + Age Effects Model",
                                     RMSE = model_m_u_a_rmse))
rmse_results %>% knitr::kable()
```

As expected, the difference of RMSE is only 0.00045 with additional consideration of age effect which confirms our judgment of weak effect by movie's age.

\pagebreak

# Results

The RMSE values of all the represented models are the following:

```
rmse_results %>% knitr::kable()
```

We therefore found the lowest value of RMSE that is 0.8648170.


# Discussion

So we can confirm that the final model for our project is the Regularized movie and user effect model:

$$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$$

As expected, the RMSE decreased while the model increases complexity. We can even discuss if a good alternative is to increase the parameters of the models like inlude genres, however to split the genres in the data we need more advanced hardware. 

# Conclusion

Machine learning algorithm was built to predict movie ratings with MovieLens dataset.

The Regularized movie and user effect model is characterized by the lower RMSE value and is hence the optimal model to use for the present project.

We could also say that some improvements in the RMSE could be achieved by adding other parameters (genre, year, age). Other different machine learning models could also improve the results further, but hardware limitations, as the RAM, are a constraint.
