---
title: "EDX Capstone MovieLens Report"
author: "Wanjun Ling"
date: "7/12/2021"
output:
  pdf_document:
    toc: yes
    fig_caption: yes
    number_sections: yes
  word_document:
    toc: yes
    fig_caption: yes
  html_document:
    toc: yes
    df_print: paged
editor_options:
  chunk_output_type: console
---
# EXECUTIVE SUMMARY

##   Introduction
The recommendation system, a subset of information filtering system which predicts users' "preference" or "rating" on an item, is widely used in the field of e-commerce business such as Amazon, JD, Netflix etc. It has become more and more popular in various data science applications since the first Netflix $1M prize challenge in 2006. 

##   Data Set
In this project a 10M movielens data set generated by the GroupLens research lab will be used as source data. 

##   Objective  
During this project, the training of the algorithms and modeling are expected. The Root Mean Square Error (short for RMSE) will be used to evaluate the accuracy which is the closeness of multiple predictions to the true values in the validation set. The formula of computing RMSE is sqrt(mean((true_ratings - predicted_ratings)^2)). The objective of this Movielens project is to create a well performed rating recommendation system through machine learning training. The goal of this project is to train an algorithm with RMSE less than 0.86490.

##   Key Steps
The key steps executed in this project includes:

1. DATA PREPARATION: Download the MovieLens 10M data set through "http://files.grouplens.org/datasets/movielens/ml-10m.zip" and create training data set "edx" and final hold-out test data set "validation".

2. EXPLORATORY ANALYSIS: Collect data set statistics, analyze, visualize and determine potential effects or biases on ratings caused by features such as MovieID, UserID, Time, Genres etc.

3. MODELING: Build multiple models with consideration of rating effects from various combinations of features such as average rating, MovieID, UserID, Time, Genres etc and then evaluate RMSEs of these models.

4. CONCLUSION: Draw a conclusion on the best model as well as the limitation of these models based on modeling results. Then suggest further research directions.

#  METHODS

##  Data Preparation
Download MovieLens 10M data set through "http://files.grouplens.org/datasets/movielens/ml-10m.zip" and create training data set "edx" and final hold-out test data set "validation". After the required data sets are created, remove the temporary files and objects from the working directory.

```{r echo=FALSE, message=FALSE, warning=FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################
# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
#check any missing value in ratings at the beginning so that we can know whether there is any issue on source data. -- for debug purpose
#anyNA(ratings)
#head(ratings)
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
#check any missing value in movies at the beginning so that we can know whether there is any issue on source data.  -- for debug purpose
#anyNA(movies)
#head(movies)
# if using R 3.6 or earlier:
#movies <- as.data.frame(movies) %>% mutate(movieId = #as.numeric(levels(movieId))[movieId],
#                                            title = as.character(title),
#                                            genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                            title = as.character(title),
                                            genres = as.character(genres))
#check any missing value in movies so that we can know whether there is any issue after first mutate action.  -- for debug purpose
#anyNA(movies)
#head(movies)
movielens <- left_join(ratings, movies, by = "movieId")
#check any missing value in movielens so that we can know whether there is any issue after first left_join.  -- for debug purpose
#anyNA(movielens)
#head(movielens)
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
#remove the temporary files and objects from the working directory
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

##   Data Exploration

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Load extra libraries for data analysis and visualization
if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
library(dslabs)
library(ggplot2)
library(lubridate)
```

After the data is loaded, we start the work by examining the data structure, data classifications and corresponding statistics of data set for better understanding of source data.

First check whether there is any missing value in data sets "edx" and "validation".

```{r message=FALSE, warning=FALSE}
#check any missing value in training data set "edx"
anyNA(edx)
#check any missing value in final hold-out test data set "validation"
anyNA(validation)
#list amount of observations and variables in training data set "edx"
dim(edx)
#list amount of observations and variables in final hold-out test data set "validation"
dim(validation)
```

The results of above checking indicate that no invalid or missing value in both data set "edx" and "validation". We observe that there are total 9,000,055 observations with 6 features in "edx" data set and 999,999 observations with 6 features in "validation" data set. Now we need to know what features are they and statistics of them.

Now we further explore the data set "edx" to know more about it.

Here are first 6 observations of data set "edx" for us to have preliminary view.
```{r echo=FALSE, message=FALSE, warning=FALSE}
head(edx)
```

Here is statistics of data set "edx".
```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(edx)
```

Now we know data set "edx" has initially 6 features of "userId", "movieId", "rating", "timestamp", "title" and "genres". Meanwhile, We notice that there is year information in the feature "title" which may need to be split for further analysis. Also, we may need to convert "timestamp" to rated year for further checking the time effect on rating. Moveover we notice that there are several types in feature "genres" for one single observation. We may need to split "genres" for better handling the genres effect on rating in upcoming data processing part. Now we know the average rating is 3.512 and minimum rating is 0.5.

##    Data Pre-processing
Before we look into more detailed data analysis, we will conduct minor data pre-processing on both "edx" and "validation" data sets. The data pre-processing tasks include converting feature "timestamp" to year rated, extracting year released from feature "title", calculating ages between year rated and year released and adding these 3 new features to original data sets.

```{r message=FALSE, warning=FALSE}
# convert timestamp to year rated and add it to edx
edx <- edx %>% mutate(year_rated = year(as_datetime(timestamp)))
# double check any invalid value after convertion of timestamp
unique(edx$year_rated)
# extract the year released from title and add it to edx
edx <- edx %>% mutate(year_released = as.numeric(str_sub(title,-5,-2)))
# double check any invalid value of year released
unique(edx$year_released)
# calculate the movie age when movie was rated and add it to edx
edx <- edx %>% mutate(ages = as.numeric(year_rated)-as.numeric(year_released))
# double check any invalid value of ages
unique(edx$ages)
```
We can see the result of data pre-processing is good except that 2 odd values of ages (-1 and -2) are observed. We are not sure whether the movies were rated before they were final released or they were just some data errors. Anyway, we plan to further check how much portion of observations with these odd values. 

```{r message=FALSE, warning=FALSE}
sum(edx$ages == -1)/nrow(edx)
sum(edx$ages == -2)/nrow(edx)
```

Now we know the portion of observations with odd values is quite small so it would not have big impact on our modeling and predication. Hence we just leave them as they are. 

Now we apply the same data pre-processing on data set "validation" too.

```{r message=FALSE, warning=FALSE}
# do the same data pre-processing for validation set
validation <- validation %>% mutate(year_rated = year(as_datetime(timestamp)))
validation <- validation %>% mutate(year_released = as.numeric(str_sub(title,-5,-2)))
validation <- validation %>% mutate(ages = as.numeric(year_rated)-as.numeric(year_released))
```

##   Data Analysis
After data pre-processing, we start to further analyze each feature for the evaluation of possible effects on our goal "rating". 

```{r message=FALSE, warning=FALSE}
# Number of unique users, movies
edx %>% summarize(unique_users = n_distinct(userId), unique_movies = n_distinct(movieId))
```

From the result of above script, we can see there are 69878 unique users and 10677 unique movies. If we simply multiply unique_users 69878 by unique_movies 10677, we'll get over 746 million records while there are only around 9 million observations in edx data set.The ratings in edx data set only count around 1.2% of all possible ratings. This huge gap implies that each user does not rate all movies. If we think in terms of a large matrix, with user on the rows and movies on the columns, the matrix will be a super sparse one. The sparsity of the matrix will be our challenge to our prediction. We will verify it in following analysis part.

First, let us take a look of the feature "rating" and it's distribution since it is our goal.

List all unique values of feature "rating".
```{r message=FALSE, warning=FALSE}
# list all rating values
unique(edx$rating)
```
We can see that there are total 10 unique ratings ranging from 0.5 to 5 with interval of 0.5. There is no rating with value 0. Let us further check the rating distributions of these 10 values.

Check which rating values are the top 10 ratings.

```{r echo = FALSE, message=FALSE, warning=FALSE}
# Top ten ratings
edx %>% group_by(rating) %>% 
  summarize(rating_sum_number = n()) %>% top_n(10) %>% arrange(desc(rating_sum_number))
```

Now plot rating counts for each rating value

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Plot rating counts for each rating value
edx %>%
  group_by(rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = rating, y = count)) + geom_vline(xintercept = mean(edx$rating), col = "blue", linetype = "dashed") +
  ggtitle("Rating Values Count") + theme(plot.title = element_text(hjust = 0.5)) + labs(subtitle ="number of ratings by UserId", x="Rating Values" , y="Rating Counts") + theme(plot.subtitle = element_text(hjust = 1)) + geom_line()
```

From the graph above, it is observed that the most popular rating is 4, followed by rating values 3, 5, 3.5, 2 etc. In general, half star ratings are much less common than whole star ratings. We also show the average rating in the histogram by a blue dashed line. The graph tells that most users tend to give whole star rating rather than half star one.

After analyzing feature "rating" itself, we need further check other features in edx data set to identify whether any effect or bias which may exist on movies' rating preference. Now let us explore whether movie bias exists.

Plot histogram on number of ratings by "movieId".

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Explore whether movies bias exists
# Plot histogram on number of ratings by "movieId"
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 10, color = "yellow") + 
  scale_x_log10() + ggtitle("Movies Rating Distributions") + theme(plot.title = element_text(hjust = 0.5)) + labs(subtitle ="number of ratings by movieId", x="movieId" ,y="number of ratings") + theme(plot.subtitle = element_text(hjust = 1)) 
```

The visualization of the number of ratings by "movieId" indicates that some movies got much more ratings than others. Some movies received over 20,000 ratings while some only got around 150 ratings. Hence movies bias actually exists. We need take movies bias into consideration in training machine learning algorithms.

Then we explore whether user bias exists.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Explore whether users bias exists
# plot histogram of number of ratings by "userId"
edx %>% count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 20, color = "green") + 
  scale_x_log10() + ggtitle("Users Rating Distributions") + theme(plot.title = element_text(hjust = 0.5)) + labs(subtitle ="number of ratings by UserId", x="userId" , y="number of ratings") + theme(plot.subtitle = element_text(hjust = 1))
```

The visualization of the number of ratings by "userId" shows that some users are much more active or preferring than others at rating movies. Again users bias actually exists. We need take users bias too into consideration in training machine learning algorithms.

Next we need explore whether time bias exists.

We explore effects of year_rated on rating.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# explore whether time bias exists
# Average Rating versus year_rated of movies
edx %>% group_by(year_rated) %>%
summarize(rating = mean(rating)) %>%
ggplot(aes(year_rated, rating)) +
geom_point() + geom_smooth() + geom_hline(yintercept = mean(edx$rating), col = "red", linetype = "dashed") + ggtitle("Year Rated Effect") + theme(plot.title = element_text(hjust = 0.5)) + labs(subtitle = "Year_rated VS Average ratings", x="Year Rated" , y="Average Ratings") + theme(plot.subtitle = element_text(hjust = 1))
```

This plot shows that people who rated movies between year 1995 and 2000 usually gave higher rating compared to the ones who rated after year 2000.

Now we explore effects of year_released on rating.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Average Rating versus year_released of movies
edx %>% group_by(year_released) %>%
summarize(rating = mean(rating)) %>%
ggplot(aes(year_released, rating)) +
geom_point() + geom_smooth(size = 5) +
geom_hline(yintercept = mean(edx$rating), col = "red", linetype = "dashed") +
ggtitle("Year Released Effect") + theme(plot.title =element_text(hjust = 0.5)) + labs(subtitle = "Year_released VS Average ratings", x="Year Released" , y="Average Ratings") + theme(plot.subtitle = element_text(hjust = 1))
```

This plot shows that older movies tended to have higher ratings. Movies released before year of 1980 generally were given higher ratings much above the average rating indicated by red dashed line.

We further explore the movies age effect on rating.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Visualize amounts of "ages VS rating"
ggplot(data = edx) + aes(x=ages,fill=rating) + geom_histogram(binwidth =5) + ggtitle("Ages Rating Distributions") + theme(plot.title = element_text(hjust = 0.5))
```

This Ages Rating Distributions plot shows that most of ratings were given to movies within ages of 15 years. Meanwhile only very few ratings were given for movies with age of over 50 years.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Average Rating versus age of movies by geom_line
edx %>% group_by(ages) %>%
summarize(rating = mean(rating)) %>%
ggplot(aes(ages, rating)) +
geom_line(color = "green") + 
scale_x_continuous(breaks=seq(0, 100, by= 5)) + geom_smooth() +
geom_hline(yintercept = mean(edx$rating), col = "red", linetype = "dashed") +
ggtitle("Movie Ages Effect") + theme(plot.title = element_text(hjust = 0.5)) +
labs(subtitle = "Movies Age VS Average ratings", x="Movies Age" , y="Average Ratings") + theme(plot.subtitle = element_text(hjust = 1))
# Average Rating versus ages of movies by geom_point
edx %>% group_by(ages) %>%
summarize(rating = mean(rating)) %>%
ggplot(aes(ages, rating)) + geom_point() + geom_smooth() +
geom_hline(yintercept = mean(edx$rating), col = "red", linetype = "dashed") +
ggtitle("Movie Ages Effect") + theme(plot.title = element_text(hjust = 0.5)) +
labs(subtitle = "Movies Age VS Average Ratings", x="Movies Age" , y="Average Ratings") + theme(plot.subtitle = element_text(hjust = 1))
```

The above plots show that movies with age between 20 to 80 usually were given higher ratings.

Let us summarize our findings based on above time effect analysis. Although new movies with ages less than 15 years usually got the most ratings, older movies were more likely to be given higher ratings which were much above average rating. This could due to new movies got more viewers compared with old movies while old good movies got more reputations with the time flies. This implicit finding suggests that time may have an effect on the average ratings. While the smooth trend of the average ratings versus year dated and ages of movies shows that time effect on the average ratings would be weak. Anyway, we will consider to take time effect into consideration in modeling part to verify time effect. 

Here we complete the time effect analysis. Now we will move to analysis of genres effect.

Let us take a look of Top 10 title and genres which contributes most of ratings.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Top 10 rated title and genres counting majority of ratings
edx %>% group_by(title, genres) %>%summarize(count=n()) %>%top_n(10,count) %>%arrange(desc(count))
```

We observe that one movie may have more than one genres. Are movies with specific genres rated much more than others? To answer this question, we need split the genres before proceeding the analysis.

```{r }
# Explore whether "genres" bias exists
# split genres in "edx" and create a new data set "edx_split_genres" for analysis
edx_split_genres <- edx %>% separate_rows(genres, sep ="\\|")
head(edx_split_genres)
```

Let us check the numbers of movies of each genres first.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Plot moviecounts per Genre
edx_split_genres %>% group_by(genres) %>% summarize(movie_num_per_genres = n_distinct(movieId)) %>% arrange(-movie_num_per_genres) %>% ggplot(aes(reorder(genres, movie_num_per_genres), movie_num_per_genres, fill= movie_num_per_genres)) + geom_bar(stat = "identity") + coord_flip() + ggtitle("Movie Counts Per Genres") + labs(x = "Split_Genres", y = "Movie_counts")
  
```

The graph above shows clearly that most of movies were with genre of "Drama" while only few were with genre of "IMAX". The top five genres are Drama, Comedy, Thriller, Romance and Action.

Then we plot rating counts for each genre.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Plot Ratingcounts per Genre 
edx_split_genres %>% 
  group_by(genres) %>%
  summarize(ratingcounts = n()) %>%
  arrange(desc(ratingcounts)) %>% ggplot(aes(reorder(genres,ratingcounts),ratingcounts)) + 
  geom_bar(aes(fill = genres),stat = "identity") + 
ggtitle("Distribution of Genres by Ratingcounts") + labs(x = "Genres", y = "Rating_counts") + theme(axis.text.x  = element_text(angle= 90))
```

This graph shows the number of ratings in each genres. The movies with genres "Drama" were rated the most while the ones with genres "IMAX" were the least rated. However, we are not able to draw a conclusion that people prefer to rate the Drama movies over other types because this could due to more movies in "Drama" genre as indicated in previous graph. 

So in order to identify real genres effect, we need further analyze average rating for each genre.

Plot Mean Rating per Genre compared with general average rating.
```{r echo=FALSE, message=FALSE, warning=FALSE}
#Average rating for each genre
Genres_avg_rating <- edx_split_genres %>%
  group_by(genres) %>%
  summarize(mean_rating_by_genre=mean(rating)) %>%
  arrange(-mean_rating_by_genre)
Genres_avg_rating
#General average rating for genres
avg_rating <- edx_split_genres %>% summarize(mean(rating))
avg_rating 
#mean(rating) 3.53
#Plot Mean Rating per Genre compared with general average rating
Genres_avg_rating %>%
  ggplot(aes(reorder(genres, mean_rating_by_genre), mean_rating_by_genre, fill= mean_rating_by_genre)) + geom_bar(stat = "identity") + coord_flip() + ggtitle("Average Rating of Per Genre") + scale_fill_distiller(palette="RdYlGn") + labs(y = "Mean Rating", x = "Genre") + geom_hline(yintercept = 3.53, col = "red", linetype = "dashed") 
```

The graph shows that some genres such as "Film_Noir" and "Documentary" have higher ratings than the general average ratings while others such as "Comedy" and "Action" have lower ratings than the general average ratings. With consideration of rating counts per genres, the genre effect seems to exist too. Hence the genre effect will also be considered in modeling part.

So far, we have analyzed the potential effects on rating by movie, user, age and genres. Now we will move to modeling stage with the analysis results.

# MODELING AND RESULTS

To measure the model accuracy, first we need define RMSE short for residual mean squared error.

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

We start by building the simplest possible recommendation system: we predict the same rating for all movies regardless of user. We know that the estimate that minimizes the RMSE is the least squares estimate of mu_hat, the average of all ratings.

##   Model 1: Only by average rating

```{r message=FALSE, warning=FALSE}
#the simplest possible model: we predict the same rating for all movies regardless of user.
mu_hat <- mean(edx$rating)
mu_hat
naive_rmse <- RMSE(validation$rating, mu_hat)
naive_rmse
```

Now we build a data frame rmse_results to store models results

```{r message=FALSE, warning=FALSE}
rmse_results <- data_frame(Model = "Only by average rating", RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```

Here we get the RMSE 1.0612 of our first model "Only by average rating". We can see RMSE of this model is quite far away from our target 0.8649. Now we add movieId to build the second model.

## Model 2: Modeling with movieId effect

Since we are already aware that movieId feature could obviously affect the ratings of a movie in the section of data analysis, we add the bias of movie (b_m) to the model. Now we plot the distribution of the bias and calculate the RMSE of this model.

```{r message=FALSE, warning=FALSE}
#calculating and plot b_m distribution
movie_effects <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_m = mean(rating - mu_hat))
movie_effects %>% qplot(b_m, geom ="histogram", bins = 10, data = ., color = I("blue"))
#calculating predicted ratings
predicted_ratings_m <- mu_hat + validation %>% 
  left_join(movie_effects, by='movieId') %>%
  pull(b_m)
#calculating model_m_rmse with consideration of movie effects
model_m_rmse <- RMSE(validation$rating,predicted_ratings_m) 
model_m_rmse
#insert new result into DF rmse_result
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="Movie Effect Model",  
                                     RMSE = model_m_rmse))
rmse_results %>% knitr::kable()
```
Now the new RMSE by adding movieId drops from 1.0612 to 0.9439. The movie effect model improves predication accuracy around 11% compared with "Only by average rating" model. The result confirms movie effect.

We will build the third model by adding combination of movieId and userId since we have already known that user bias also exists.

## Model 3: Modeling with movieId + userId effects

Similar to the movie effect, we now further add the bias of user effect (b_u) to the movie effect model.

```{r message=FALSE, warning=FALSE}
#calculating and plot b_u distribution
user_effects <- edx %>% 
  left_join(movie_effects, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu_hat - b_m))
user_effects %>% qplot(b_u, geom ="histogram", bins = 10, data = ., color = I("blue")) 
#calculating predicted ratings
predicted_ratings_m_u <- validation %>% 
  left_join(movie_effects, by='movieId') %>%
  left_join(user_effects, by='userId') %>%
  mutate(prediction = mu_hat + b_m + b_u) %>%
  pull(prediction)
#calculating model_m_rmse with consideration of movie + user effects  
model_m_u_rmse <- RMSE(validation$rating,predicted_ratings_m_u)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="Movie + User Effects Model",  
                                     RMSE = model_m_u_rmse))
rmse_results %>% knitr::kable()
```

Again, the new RMSE by adding movieId and userId together drops from 0.9439 to 0.8653. The movie and user effect model further improves predication accuracy around 18.5% compared with "Only by average rating" model.

In data analysis part, we estimate the time effect would be weak. What will new RMSE be by adding movie age on top of movie and user effect?

## Model 4: Modeling with movieId + userId + movieAges effects
We now further add the bias of movie age effect (b_a) to the movie + user effect model.

```{r message=FALSE, warning=FALSE}
#calculating and plot b_a distribution
ages_effects <- edx %>% 
  left_join(movie_effects, by='movieId') %>%
  left_join(user_effects, by='userId') %>%
  group_by(ages) %>%
  summarize(b_a = mean(rating - mu_hat - b_m - b_u))
ages_effects %>% qplot(b_a, geom ="histogram", bins = 10, data = ., color = I("blue"))
#calculating predicted ratings
predicted_ratings_m_u_a <- validation %>% 
  left_join(movie_effects, by='movieId') %>%
  left_join(user_effects, by='userId') %>%
  left_join(ages_effects, by='ages') %>%
  mutate(prediction = mu_hat + b_m + b_u + b_a) %>%
  pull(prediction)
#calculating model_m_rmse with consideration of movie + user + age effects  
model_m_u_a_rmse <- RMSE(validation$rating,predicted_ratings_m_u_a)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="Movie + User + Age Effects Model",
                                     RMSE = model_m_u_a_rmse))
rmse_results %>% knitr::kable()
```

As expected, the difference of RMSE is only 0.00045 with additional consideration of age effect which confirms our judgment of weak effect by movie's age.

Now we will build a new model by regularization of movie and user effects since age effect is weak.

## Model 5: Regularization of movieId + userId effects 
The general idea behind regularization is to constrain the total variability of the effect size. As lambda is a tuning parameter, we can use cross-validation to choose optimized one for model building and performance evaluation.

```{r message=FALSE, warning=FALSE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
mu_reg <- mean(edx$rating)
#regulation movie effect
b_m_reg <- edx %>%
group_by(movieId) %>%
summarize(b_m_reg = sum(rating - mu_reg)/(n()+l))
#regulation user effect
b_u_reg <- edx %>%
left_join(b_m_reg, by="movieId") %>%
group_by(userId) %>%
summarize(b_u_reg = sum(rating - b_m_reg - mu_reg)/(n()+l))
#calculating predicted ratings
predicted_ratings_b_m_u <-
validation %>%
left_join(b_m_reg, by = "movieId") %>%
left_join(b_u_reg, by = "userId") %>%
mutate(prediction = mu_reg + b_m_reg + b_u_reg) %>%
.$prediction
return(RMSE(validation$rating,predicted_ratings_b_m_u))
})
qplot(lambdas, rmses)
#For the full model, the optimal lambda is given as
lambda <- lambdas[which.min(rmses)]
lambda
#calculating regularization model RMSE: model_m_u_reg_rmse
model_m_u_reg_rmse <- min(rmses)
model_m_u_reg_rmse
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="Movie + User Regularization Model",
                                     RMSE = model_m_u_reg_rmse))
rmse_results %>% knitr::kable()
```

Now we finally achieve the goal of RMSE which is less than 0.86490 by model of regularization of movie and user effects. The new RMSE is 0.8648. 

Sa far, we have not tried to add genres effect in our model yet. Since genres also has effect on rating, we will build the sixth model to combine genres effect.

## Model 6: Modeling with movieId + userId + movieAges + genres effects
We now further add the bias of genres effect (b_g) to the movie + user + ages effect model.

#```{r fig = TRUE, message=FALSE, warning=FALSE}
```{r }
# Create data set validation_split_genres with split genres from data set validation 

validation_split_genres <- validation %>% separate_rows(genres, sep ="\\|")

# Chunk options
knitr::opts_chunk$set(
 fig.width = 6,
 fig.asp = 0.8,
 out.width = "80%"
)

mu_hat_s <- mean(edx_split_genres$rating)

movie_effects_s <- edx_split_genres %>% 
  group_by(movieId) %>% 
  summarize(b_m_s = mean(rating - mu_hat_s))

user_effects_s <- edx_split_genres %>% 
  left_join(movie_effects_s, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u_s = mean(rating - mu_hat_s - b_m_s))

ages_effects_s <- edx_split_genres %>% 
  left_join(movie_effects_s, by='movieId') %>%
  left_join(user_effects_s, by='userId') %>%
  group_by(ages) %>%
  summarize(b_a_s = mean(rating - mu_hat_s - b_m_s - b_u_s))

#calculating and plot b_g distribution
genres_effects_s <- edx_split_genres %>% 
  left_join(movie_effects_s, by='movieId') %>%
  left_join(user_effects_s, by='userId') %>%
  left_join(ages_effects_s, by='ages') %>%
  group_by(genres) %>%
  summarize(b_g_s = mean(rating - mu_hat_s - b_m_s - b_u_s - b_a_s))
genres_effects_s %>% qplot(b_g_s, geom ="histogram", bins = 10, data = ., color = I("blue"))

#calculating predicted ratings
predicted_ratings_m_u_a_g_s <- validation_split_genres %>% 
  left_join(movie_effects_s, by='movieId') %>%
  left_join(user_effects_s, by='userId') %>%
  left_join(ages_effects_s, by='ages') %>%
  left_join(genres_effects_s, by='genres') %>%
  mutate(prediction = mu_hat_s + b_m_s + b_u_s + b_a_s + b_g_s) %>%
  pull(prediction)

#calculating model_m_rmse with consideration of movie + user + age effects  
model_m_u_a_g_s_rmse <- RMSE(validation_split_genres$rating,predicted_ratings_m_u_a_g_s)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="Movie + User + Age + Genres Effects Model",
                                     RMSE = model_m_u_a_g_s_rmse))
rmse_results %>% knitr::kable()
```
Now we further improve the predication accuracy with RMSE 0.8627469 by modeling with movieId + userId + movieAges + genres effects. Just like age effect, the genres effect is weak too. So far, compared with the first model "Only by average rating", RMSE of this model drops around 19%. 
Comparing model 3 (Movie + User Effects Model) and model 5(Movie + User Regularization Model), we find that there is not much improvement with regularization. The RMSE by regularization only drops 0.0005 which is quite limited. We suspect it may be caused by sparsity issue mentioned in the section of 2.4 Data Analysis.
# CONCLUSION
## Conclusion
This MovieLens data science project enhance students' understanding of all 8 courses greatly. To achieve the project goal of generating RMSE less than 0.86490, total six models and machine learning algorithms have been successfully developed and trained with results as follows.
```{r echo=FALSE}
rmse_results %>% knitr::kable()
```

When looking at the first model "Only by average rating", we are not surprising to the bad prediction with RMSE 1.06120 as our assumption of same rating for all movies does not reflect the real world situation. With consideration of movie effects on rating, we improve our accuracy around 11% with RMSE 0.94390. Further by adding user effects, we improve our accuracy around 18% compared with "Only by average rating" model. Then, by using the penalized least squares approach, regulation of movie and user effects, we successfully improve our accuracy with RMSE 0.86481 which is less than 0.86490. Last, by considering all effects caused by four features movieId, userId, ages and genres together, we successfully improve our accuracy with RMSE 0.86274.

However, we notice that there is very limited improvement (from 0.8654 to 0.8648) by introducing regularization model. The reason behind is the sparsity challenge of MovieLens data set. If we think in terms of a large matrix, with user on the rows and movies on the columns, we'll get a large sparse matrix containing many empty cells. Obviously, all 6 models and machine learning algorithms developed till now in this project have big limitations on sparse data matrix. 

## Future Work
We need more powerful machine learning algorithms and techniques to design a recommendation system such as Matrix factorization which is very much related to factor analysis, singular value decomposition (SVD), and principal component analysis (PCA). 

Another method to be considered for future work could be Ensembles which can usually greatly improve the final results by combining the results of different algorithms.
